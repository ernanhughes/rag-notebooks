{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import requests\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\" # local ollama url\n",
    "MODEL_NAME = \"llama3.2:latest\" # model we are using to chat\n",
    "EMBEDDING_MODEL_NAME = \"bge-m3:latest\" # model we are using to embed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(document_list:list, model_name:str=\"bge-m3:latest\") -> list[torch.FloatTensor]:\n",
    "    embeddings = []\n",
    "    for document in document_list:\n",
    "        embedding_list = _generate_embeddings(document, model_name)\n",
    "        embedding = torch.FloatTensor(embedding_list)\n",
    "        embeddings.append(embedding)\n",
    "    assert len(embeddings) == len(document_list)\n",
    "    return torch.stack(embeddings, dim=0)\n",
    "\n",
    "def _generate_embeddings(document:str, model_name:str=\"bge-m3:latest\") -> list[float]:\n",
    "    \"\"\"Generate embeddings for the given text using the specified model.\"\"\"\n",
    "    try:\n",
    "        logging.debug(\"Generating embeddings for {}\".format(document))\n",
    "        # Send a POST request to generate embeddings\n",
    "        url = f\"{OLLAMA_BASE_URL}/api/embeddings\"\n",
    "        data = {\n",
    "            \"prompt\": document,\n",
    "            \"model\": model_name\n",
    "        }\n",
    "        response = requests.post(url, json=data)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON response\n",
    "            embeddings = response.json()[\"embedding\"]\n",
    "            return embeddings\n",
    "        else:\n",
    "            logging.error(f\"Failed to generate embeddings. Status code: {response.status_code}\")\n",
    "            logging.error(\"Response:\", response.text)\n",
    "            return []\n",
    "    \n",
    "    except requests.ConnectionError:\n",
    "        logging.error(\"Failed to connect to the Ollama server. Make sure it is running locally and the URL is correct.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        logging.error(\"Failed to parse JSON response from Ollama server.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found [-1 -1] documents\n",
      "Retrieved Documents: ['Natural language processing (NLP) is a field of AI that focuses on the interaction between computers and human language.', 'Natural language processing (NLP) is a field of AI that focuses on the interaction between computers and human language.']\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"Python is a programming language that lets you work quickly.\",\n",
    "    \"Machine learning is a method of data analysis that automates analytical model building.\",\n",
    "    \"Artificial Intelligence (AI) is intelligence demonstrated by machines.\",\n",
    "    \"Natural language processing (NLP) is a field of AI that focuses on the interaction between computers and human language.\",\n",
    "]\n",
    "\n",
    "# Generate embeddings for documents\n",
    "document_embeddings = generate_embeddings(documents)\n",
    "logging.info(f\"Generated Document Embeddings: {document_embeddings.shape}\")\n",
    "\n",
    "# Build a FAISS index for fast similarity search\n",
    "index = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
    "\n",
    "\n",
    "def retrieve_documents(query, top_k=2):\n",
    "    # Generate query embedding\n",
    "    query_embedding = generate_embeddings([query])\n",
    "    # Search for the most similar documents\n",
    "    _, indices = index.search(np.array(query_embedding), top_k)\n",
    "    # Return the retrieved documents\n",
    "    print(f\"Found {indices[0]} documents\")\n",
    "    return [documents[i] for i in indices[0]]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "query = \"Tell me about artificial intelligence\"\n",
    "retrieved_docs = retrieve_documents(query)\n",
    "print(\"Retrieved Documents: {}\".format(retrieved_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"Retrieved Documents: {}\".format(retrieved_docs))\n",
    "for doc in retrieved_docs:\n",
    "    logging.info(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_curl(json_data, base_url):\n",
    "    curl_command = \"curl -X POST -H \\\"Content-Type: application/json\\\" -d '\" + json.dumps(json_data) + \"' \" + base_url + \"/api/chat\"\n",
    "    return curl_command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat_with_ollama(prompt, model_name =\"llama3.2:latest\"):\n",
    "    try:\n",
    "        url = f\"{OLLAMA_BASE_URL}/api/chat\"\n",
    "        data = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"stream\" : False\n",
    "        }\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"message\"][\"content\"]\n",
    "        else:\n",
    "            logging.error(f\"Failed to generate response. Status code: {response.status_code}\")\n",
    "            logging.error(\"Response:\", response.text)\n",
    "            return None\n",
    "    \n",
    "    except requests.ConnectionError:\n",
    "        logging.error(\"Failed to connect to the Ollama server. Make sure it is running locally and the URL is correct.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        logging.error(\"Failed to parse JSON response from Ollama server.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    formatted_docs = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        formatted_docs += f\"{i+1}. {doc}\\n\"\n",
    "    return formatted_docs\n",
    "\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    # Combine the query with the retrieved documents to form the context\n",
    "    context = f\"Using this information :\\n { format_docs(retrieved_docs) }\\n Can you answer this question: {query}.\"\n",
    "    print(f\"Generated Context:\\n {context}\")\n",
    "\n",
    "    # Return the generated answer\n",
    "    return chat_with_ollama(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_system(query):\n",
    "    # Retrieve relevant documents based on the query\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    # Generate a response using the retrieved documents as context\n",
    "    generated_answer = generate_answer(query, retrieved_docs)\n",
    "    \n",
    "    return generated_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found [-1 -1] documents\n",
      "Generated Context:\n",
      " Using this information :\n",
      " 1. Natural language processing (NLP) is a field of AI that focuses on the interaction between computers and human language.\n",
      "2. Natural language processing (NLP) is a field of AI that focuses on the interaction between computers and human language.\n",
      "\n",
      " Can you answer this question: What is Natural Language Processing?.\n",
      "A nice example of repetition!\n",
      "\n",
      "According to the given information, Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) that focuses on the interaction between computers and human language.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Natural Language Processing?\"\n",
    "\n",
    "result = rag_system(query)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
