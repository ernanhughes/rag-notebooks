{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\rag-notebooks\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "filtered_knowledge_base = knowledge_base.filter(\n",
    "    lambda row: row[\"source\"].startswith(\"huggingface/transformers\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\rag-notebooks\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                                                                                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">For a transformers model training, which is slower, the forward or the backward pass?</span>                                                                                                                <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                                                                                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•°â”€ LiteLLMModel - ollama/qwen2.5-coder:14b â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ•­â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•®\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                                                                                                      \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mFor a transformers model training, which is slower, the forward or the backward pass?\u001b[0m                                                                                                                \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                                                                                                      \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ•°â”€\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5-coder:14b \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m0\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: To determine whether the forward or backward pass in Transformers model training is slower, I need to first understand what these terms mean in the context of neural network training. The </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">forward pass involves passing input data through the model to get predictions, while the backward pass (also known as backpropagation) involves computing gradients to update the model weights.</span><span style=\"background-color: #0d1117\">        </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                                                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">To get a detailed understanding and comparison, I will use the </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`retriever`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> tool to find relevant information from the Transformers documentation.</span><span style=\"background-color: #0d1117\">                                                       </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                                                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                   </span>\n",
       "<span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">forward_backward_info </span><span style=\"color: #ff7b72; text-decoration-color: #ff7b72; background-color: #0d1117; font-weight: bold\">=</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> retriever(query</span><span style=\"color: #ff7b72; text-decoration-color: #ff7b72; background-color: #0d1117; font-weight: bold\">=</span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">\"Transformer forward and backward pass\"</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">)</span><span style=\"background-color: #0d1117\">                                                                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(</span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">\"Information about Transformer forward and backward pass:\"</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">, forward_backward_info)</span><span style=\"background-color: #0d1117\">                                                                                                                </span>\n",
       "<span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">```</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                     </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mOutput message of the LLM:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdetermine\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhether\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mforward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformers\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mslower,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mneed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfirst\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23munderstand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthese\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mterms\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmean\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcontext\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mof\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mneural\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnetwork\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mThe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mforward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minvolves\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpassing\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minput\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdata\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthrough\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mget\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpredictions,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhile\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(also\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mknown\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mas\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackpropagation)\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minvolves\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomputing\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgradients\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mupdate\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mweights.\u001b[0m\u001b[48;2;13;17;23m        \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mTo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mget\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23ma\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdetailed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23munderstanding\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomparison,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`retriever`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfind\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrelevant\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfrom\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformers\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocumentation.\u001b[0m\u001b[48;2;13;17;23m                                                       \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;165;214;255;48;2;13;17;23m```\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23mpy\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mforward_backward_info\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[1;38;2;255;123;114;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretriever\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mquery\u001b[0m\u001b[1;38;2;255;123;114;48;2;13;17;23m=\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23mTransformer forward and backward pass\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m)\u001b[0m\u001b[48;2;13;17;23m                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mprint\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23mInformation about Transformer forward and backward pass:\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mforward_backward_info\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m)\u001b[0m\u001b[48;2;13;17;23m                                                                                                                \u001b[0m\n",
       "\u001b[38;2;165;214;255;48;2;13;17;23m```\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                     \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> â”€ <span style=\"font-weight: bold\">Executing this code:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">forward_backward_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Transformer forward and backward pass\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                                    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Information about Transformer forward and backward pass:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, forward_backward_info)</span><span style=\"background-color: #272822\">                                                                                                            </span>  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " â”€ \u001b[1mExecuting this code:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mforward_backward_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTransformer forward and backward pass\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mInformation about Transformer forward and backward pass:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mforward_backward_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                            \u001b[0m  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Information about Transformer forward and backward pass: \n",
       "Retrieved documents:\n",
       "\n",
       "\n",
       "===== Document 0 =====\n",
       "- A train step function which combines the loss function and optimizer update, does the forward and backward pass and returns the updated parameters.\n",
       "\n",
       "===== Document 1 =====\n",
       "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in \n",
       "significant memory overhead. The alternative approach of discarding the activations and recalculating them when needed \n",
       "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
       "\n",
       "===== Document 2 =====\n",
       "overhead. This is super helpful when you have activation checkpointing enabled, where we do a forward recompute and\n",
       "backward passes a single layer granularity and want to keep the parameter in the forward recompute till the backward\n",
       "\n",
       "===== Document 3 =====\n",
       "For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates \n",
       "into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually \n",
       "bandwidth-limited, and itâ€™s typical for an activation to have to read more data in the backward than in the forward \n",
       "(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward,\n",
       "\n",
       "===== Document 4 =====\n",
       "## How to benchmark ğŸ¤— Transformers models\n",
       "\n",
       "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark ğŸ¤— Transformers models. The benchmark classes allow us to measure the _peak memory usage_ and _required time_ \n",
       "for both _inference_ and _training_.\n",
       "\n",
       "&lt;Tip&gt;\n",
       "\n",
       "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
       "backward pass.\n",
       "\n",
       "&lt;/Tip&gt;\n",
       "\n",
       "===== Document 5 =====\n",
       "**Forward Activations**\n",
       "\n",
       "- size depends on many factors, the key ones being sequence length, hidden size and batch size.\n",
       "\n",
       "There are the input and output that are being passed and returned by the forward and the backward functions and the \n",
       "forward activations saved for gradient computation.\n",
       "\n",
       "**Temporary Memory**\n",
       "\n",
       "===== Document 6 =====\n",
       "...         # forward + backward + optimize\n",
       "...         with tf.GradientTape() as tape:\n",
       "...             outputs = model(\n",
       "...                 input_ids=input_ids,\n",
       "...                 attention_mask=attention_mask,\n",
       "...                 token_type_ids=token_type_ids,\n",
       "...                 labels=labels,\n",
       "...                 numeric_values=numeric_values,\n",
       "...                 numeric_values_scale=numeric_values_scale,\n",
       "...                 float_answer=float_answer,\n",
       "...             )\n",
       "\n",
       "===== Document 7 =====\n",
       "model_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors=\"pt\")\n",
       "\n",
       "model(**model_inputs)  # forward pass\n",
       "```\n",
       "\n",
       "- Generation\n",
       "\n",
       "===== Document 8 =====\n",
       "This way each of the 3 GPUs gets the full tensors reconstructed and makes a forward pass with its own mini-batch.\n",
       "As soon as the calculation is done, the data that is no longer needed gets dropped - it's only used during the calculation. \n",
       "The reconstruction is done efficiently via a pre-fetch.\n",
       "\n",
       "Then the whole process is repeated for layer Lb, then Lc forward-wise, and then backward Lc -&gt; Lb -&gt; La.\n",
       "\n",
       "&lt;Tip&gt;\n",
       "\n",
       "===== Document 9 =====\n",
       "...         # forward + backward + optimize\n",
       "...         outputs = model(\n",
       "...             input_ids=input_ids,\n",
       "...             attention_mask=attention_mask,\n",
       "...             token_type_ids=token_type_ids,\n",
       "...             labels=labels,\n",
       "...             numeric_values=numeric_values,\n",
       "...             numeric_values_scale=numeric_values_scale,\n",
       "...             float_answer=float_answer,\n",
       "...         )\n",
       "...         loss = outputs.loss\n",
       "...         loss.backward()\n",
       "...         optimizer.step()\n",
       "```\n",
       "&lt;/pt&gt;\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Information about Transformer forward and backward pass: \n",
       "Retrieved documents:\n",
       "\n",
       "\n",
       "===== Document 0 =====\n",
       "- A train step function which combines the loss function and optimizer update, does the forward and backward pass and returns the updated parameters.\n",
       "\n",
       "===== Document 1 =====\n",
       "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in \n",
       "significant memory overhead. The alternative approach of discarding the activations and recalculating them when needed \n",
       "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
       "\n",
       "===== Document 2 =====\n",
       "overhead. This is super helpful when you have activation checkpointing enabled, where we do a forward recompute and\n",
       "backward passes a single layer granularity and want to keep the parameter in the forward recompute till the backward\n",
       "\n",
       "===== Document 3 =====\n",
       "For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates \n",
       "into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually \n",
       "bandwidth-limited, and itâ€™s typical for an activation to have to read more data in the backward than in the forward \n",
       "(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward,\n",
       "\n",
       "===== Document 4 =====\n",
       "## How to benchmark ğŸ¤— Transformers models\n",
       "\n",
       "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark ğŸ¤— Transformers models. The benchmark classes allow us to measure the _peak memory usage_ and _required time_ \n",
       "for both _inference_ and _training_.\n",
       "\n",
       "<Tip>\n",
       "\n",
       "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
       "backward pass.\n",
       "\n",
       "</Tip>\n",
       "\n",
       "===== Document 5 =====\n",
       "**Forward Activations**\n",
       "\n",
       "- size depends on many factors, the key ones being sequence length, hidden size and batch size.\n",
       "\n",
       "There are the input and output that are being passed and returned by the forward and the backward functions and the \n",
       "forward activations saved for gradient computation.\n",
       "\n",
       "**Temporary Memory**\n",
       "\n",
       "===== Document 6 =====\n",
       "...         # forward + backward + optimize\n",
       "...         with tf.GradientTape() as tape:\n",
       "...             outputs = model(\n",
       "...                 input_ids=input_ids,\n",
       "...                 attention_mask=attention_mask,\n",
       "...                 token_type_ids=token_type_ids,\n",
       "...                 labels=labels,\n",
       "...                 numeric_values=numeric_values,\n",
       "...                 numeric_values_scale=numeric_values_scale,\n",
       "...                 float_answer=float_answer,\n",
       "...             )\n",
       "\n",
       "===== Document 7 =====\n",
       "model_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors=\"pt\")\n",
       "\n",
       "model(**model_inputs)  # forward pass\n",
       "```\n",
       "\n",
       "- Generation\n",
       "\n",
       "===== Document 8 =====\n",
       "This way each of the 3 GPUs gets the full tensors reconstructed and makes a forward pass with its own mini-batch.\n",
       "As soon as the calculation is done, the data that is no longer needed gets dropped - it's only used during the calculation. \n",
       "The reconstruction is done efficiently via a pre-fetch.\n",
       "\n",
       "Then the whole process is repeated for layer Lb, then Lc forward-wise, and then backward Lc -> Lb -> La.\n",
       "\n",
       "<Tip>\n",
       "\n",
       "===== Document 9 =====\n",
       "...         # forward + backward + optimize\n",
       "...         outputs = model(\n",
       "...             input_ids=input_ids,\n",
       "...             attention_mask=attention_mask,\n",
       "...             token_type_ids=token_type_ids,\n",
       "...             labels=labels,\n",
       "...             numeric_values=numeric_values,\n",
       "...             numeric_values_scale=numeric_values_scale,\n",
       "...             float_answer=float_answer,\n",
       "...         )\n",
       "...         loss = outputs.loss\n",
       "...         loss.backward()\n",
       "...         optimizer.step()\n",
       "```\n",
       "</pt>\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 165.68 seconds| Input tokens: 2,048 | Output tokens: 131]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 0: Duration 165.68 seconds| Input tokens: 2,048 | Output tokens: 131]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought: Based on the information retrieved, it appears that the backward pass in Transformers model training is generally slower than the forward pass. This can be attributed to several factors:</span><span style=\"background-color: #0d1117\">     </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                                                                                                        </span>\n",
       "<span style=\"color: #ff7b72; text-decoration-color: #ff7b72; background-color: #0d1117\">1.</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> </span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117; font-weight: bold\">**Higher Computational Overhead**</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">: The backward pass involves more operations compared to the forward pass, particularly for layers like convolutions and linear layers where there are approximately</span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">2x flops (floating-point operations) in the backward step compared to the forward step.</span><span style=\"background-color: #0d1117\">                                                                                                                 </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                                                                                                        </span>\n",
       "<span style=\"color: #ff7b72; text-decoration-color: #ff7b72; background-color: #0d1117\">2.</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> </span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117; font-weight: bold\">**Memory Overhead**</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">: While the backward pass can be memory-intensive due to the need to save activations for gradient computation, the computational overhead often outweighs this.</span><span style=\"background-color: #0d1117\">                  </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                                                                                                        </span>\n",
       "<span style=\"color: #ff7b72; text-decoration-color: #ff7b72; background-color: #0d1117\">3.</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> </span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117; font-weight: bold\">**Gradient Calculation**</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">: The backward pass is responsible for computing gradients, which involves more complex operations compared to simply producing predictions during the forward pass.</span><span style=\"background-color: #0d1117\">         </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                                                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">To further confirm and understand the relative slowness of the backward pass, I will use the </span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">`retriever`</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> tool again to find more detailed information on benchmarking Transformer models, specifically </span><span style=\"background-color: #0d1117\"> </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">focusing on training time metrics.</span><span style=\"background-color: #0d1117\">                                                                                                                                                                      </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                                                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                   </span>\n",
       "<span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">benchmark_info </span><span style=\"color: #ff7b72; text-decoration-color: #ff7b72; background-color: #0d1117; font-weight: bold\">=</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> retriever(query</span><span style=\"color: #ff7b72; text-decoration-color: #ff7b72; background-color: #0d1117; font-weight: bold\">=</span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">\"Transformer model training benchmark\"</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">)</span><span style=\"background-color: #0d1117\">                                                                                                                                </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(</span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">\"Information about Transformer model training benchmark:\"</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">, benchmark_info)</span><span style=\"background-color: #0d1117\">                                                                                                                        </span>\n",
       "<span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">```</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                     </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">=======</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mOutput message of the LLM:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mBased\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretrieved,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mit\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mappears\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformers\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgenerally\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mslower\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthan\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mforward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mThis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcan\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mattributed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mseveral\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfactors:\u001b[0m\u001b[48;2;13;17;23m     \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;255;123;114;48;2;13;17;23m1.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[1;38;2;230;237;243;48;2;13;17;23m**Higher Computational Overhead**\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mThe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minvolves\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moperations\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcompared\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mforward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mparticularly\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlayers\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlike\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mconvolutions\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlinear\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mlayers\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhere\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthere\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mare\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mapproximately\u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m2x\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mflops\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(floating-point\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moperations)\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mstep\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcompared\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mforward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mstep.\u001b[0m\u001b[48;2;13;17;23m                                                                                                                 \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;255;123;114;48;2;13;17;23m2.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[1;38;2;230;237;243;48;2;13;17;23m**Memory Overhead**\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mWhile\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcan\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmemory-intensive\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdue\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mneed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msave\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mactivations\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgradient\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomputation,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomputational\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moverhead\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moften\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moutweighs\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthis.\u001b[0m\u001b[48;2;13;17;23m                  \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;255;123;114;48;2;13;17;23m3.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[1;38;2;230;237;243;48;2;13;17;23m**Gradient Calculation**\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mThe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mresponsible\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomputing\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mgradients,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwhich\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minvolves\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcomplex\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23moperations\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mcompared\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msimply\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mproducing\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpredictions\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mduring\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mforward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass.\u001b[0m\u001b[48;2;13;17;23m         \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mTo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfurther\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mconfirm\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23munderstand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrelative\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mslowness\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mof\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbackward\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mpass,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23muse\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m`retriever`\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtool\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23magain\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfind\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdetailed\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbenchmarking\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodels,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mspecifically\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mfocusing\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtime\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmetrics.\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                      \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;165;214;255;48;2;13;17;23m```\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23mpy\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mbenchmark_info\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[1;38;2;255;123;114;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretriever\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mquery\u001b[0m\u001b[1;38;2;255;123;114;48;2;13;17;23m=\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23mTransformer model training benchmark\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m)\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mprint\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23mInformation about Transformer model training benchmark:\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbenchmark_info\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m)\u001b[0m\u001b[48;2;13;17;23m                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;165;214;255;48;2;13;17;23m```\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                     \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m=======\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> â”€ <span style=\"font-weight: bold\">Executing this code:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">benchmark_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Transformer model training benchmark\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                                            </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Information about Transformer model training benchmark:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, benchmark_info)</span><span style=\"background-color: #272822\">                                                                                                                    </span>  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " â”€ \u001b[1mExecuting this code:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mbenchmark_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTransformer model training benchmark\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mInformation about Transformer model training benchmark:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbenchmark_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                    \u001b[0m  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Information about Transformer model training benchmark: \n",
       "Retrieved documents:\n",
       "\n",
       "\n",
       "===== Document 0 =====\n",
       "The Table Transformer model was proposed in [PubTables-1M: Towards comprehensive table extraction from unstructured documents](https://arxiv.org/abs/2110.00061) by\n",
       "Brandon Smock, Rohith Pesala, Robin Abraham. The authors introduce a new dataset, PubTables-1M, to benchmark progress in table extraction from unstructured documents,\n",
       "\n",
       "===== Document 1 =====\n",
       ". Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness \n",
       "of our approach.*\n",
       "\n",
       "===== Document 2 =====\n",
       "Distil* is a class of compressed models that started with DistilBERT. DistilBERT stands for Distilled-BERT. DistilBERT is a small, fast, cheap and light Transformer model based on Bert architecture. \n",
       "It has 40% less parameters than `bert-base-uncased`, runs 60% faster while preserving 97% of BERT's performances as measured on the GLUE language understanding benchmark\n",
       "\n",
       "===== Document 3 =====\n",
       "Instead of benchmarking pre-trained models via their model identifier, _e.g._ `bert-base-uncased`, the user can\n",
       "alternatively benchmark an arbitrary configuration of any available model class. In this case, a `list` of\n",
       "configurations must be inserted with the benchmark args as follows.\n",
       "\n",
       "&lt;frameworkcontent&gt;\n",
       "&lt;pt&gt;\n",
       "```py\n",
       "&gt;&gt;&gt; from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments, BertConfig\n",
       "\n",
       "===== Document 4 =====\n",
       "Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these\n",
       "findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the\n",
       "number of parameters at the fine-tuning stage.*\n",
       "\n",
       "===== Document 5 =====\n",
       "# Swin2SR\n",
       "\n",
       "## Overview\n",
       "\n",
       "The Swin2SR model was proposed in [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, \n",
       "Radu Timofte.\n",
       "Swin2R improves the [SwinIR](https://github.com/JingyunLiang/SwinIR/) model by incorporating [Swin Transformer v2](swinv2) layers which mitigates issues such as training instability, resolution gaps \n",
       "between pre-training\n",
       "and fine-tuning, and hunger on data.\n",
       "\n",
       "===== Document 6 =====\n",
       "&lt;/Tip&gt;\n",
       "\n",
       "The benchmark classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] expect an object of type [`PyTorchBenchmarkArguments`] and\n",
       "[`TensorFlowBenchmarkArguments`], respectively, for instantiation. [`PyTorchBenchmarkArguments`] and [`TensorFlowBenchmarkArguments`] are data classes and contain all relevant configurations for their\n",
       "corresponding benchmark class. In the following example, it is shown how a BERT model of type _bert-base-cased_ can be benchmarked.\n",
       "\n",
       "===== Document 7 =====\n",
       "# XTREME-S benchmark examples\n",
       "\n",
       "*Maintainers: [Anton Lozhkov](https://github.com/anton-l) and [Patrick von Platen](https://github.com/patrickvonplaten)*\n",
       "\n",
       "===== Document 8 =====\n",
       "## How to benchmark ğŸ¤— Transformers models\n",
       "\n",
       "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark ğŸ¤— Transformers models. The benchmark classes allow us to measure the _peak memory usage_ and _required time_ \n",
       "for both _inference_ and _training_.\n",
       "\n",
       "&lt;Tip&gt;\n",
       "\n",
       "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
       "backward pass.\n",
       "\n",
       "&lt;/Tip&gt;\n",
       "\n",
       "===== Document 9 =====\n",
       "#### Ben Wang, Independent AI Researcher, EleutherAI\n",
       "- Talk: Multihost Training in Mesh Transformer JAX\n",
       "- Abstract: As models become larger, training must be scaled across multiple nodes. This talk discusses some design decisions and tradeoffs made for scaling to multiple nodes in Mesh Transformer JAX, \n",
       "a library for running model parallel transformers on TPU pods.\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Information about Transformer model training benchmark: \n",
       "Retrieved documents:\n",
       "\n",
       "\n",
       "===== Document 0 =====\n",
       "The Table Transformer model was proposed in [PubTables-1M: Towards comprehensive table extraction from unstructured documents](https://arxiv.org/abs/2110.00061) by\n",
       "Brandon Smock, Rohith Pesala, Robin Abraham. The authors introduce a new dataset, PubTables-1M, to benchmark progress in table extraction from unstructured documents,\n",
       "\n",
       "===== Document 1 =====\n",
       ". Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness \n",
       "of our approach.*\n",
       "\n",
       "===== Document 2 =====\n",
       "Distil* is a class of compressed models that started with DistilBERT. DistilBERT stands for Distilled-BERT. DistilBERT is a small, fast, cheap and light Transformer model based on Bert architecture. \n",
       "It has 40% less parameters than `bert-base-uncased`, runs 60% faster while preserving 97% of BERT's performances as measured on the GLUE language understanding benchmark\n",
       "\n",
       "===== Document 3 =====\n",
       "Instead of benchmarking pre-trained models via their model identifier, _e.g._ `bert-base-uncased`, the user can\n",
       "alternatively benchmark an arbitrary configuration of any available model class. In this case, a `list` of\n",
       "configurations must be inserted with the benchmark args as follows.\n",
       "\n",
       "<frameworkcontent>\n",
       "<pt>\n",
       "```py\n",
       ">>> from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments, BertConfig\n",
       "\n",
       "===== Document 4 =====\n",
       "Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these\n",
       "findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the\n",
       "number of parameters at the fine-tuning stage.*\n",
       "\n",
       "===== Document 5 =====\n",
       "# Swin2SR\n",
       "\n",
       "## Overview\n",
       "\n",
       "The Swin2SR model was proposed in [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, \n",
       "Radu Timofte.\n",
       "Swin2R improves the [SwinIR](https://github.com/JingyunLiang/SwinIR/) model by incorporating [Swin Transformer v2](swinv2) layers which mitigates issues such as training instability, resolution gaps \n",
       "between pre-training\n",
       "and fine-tuning, and hunger on data.\n",
       "\n",
       "===== Document 6 =====\n",
       "</Tip>\n",
       "\n",
       "The benchmark classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] expect an object of type [`PyTorchBenchmarkArguments`] and\n",
       "[`TensorFlowBenchmarkArguments`], respectively, for instantiation. [`PyTorchBenchmarkArguments`] and [`TensorFlowBenchmarkArguments`] are data classes and contain all relevant configurations for their\n",
       "corresponding benchmark class. In the following example, it is shown how a BERT model of type _bert-base-cased_ can be benchmarked.\n",
       "\n",
       "===== Document 7 =====\n",
       "# XTREME-S benchmark examples\n",
       "\n",
       "*Maintainers: [Anton Lozhkov](https://github.com/anton-l) and [Patrick von Platen](https://github.com/patrickvonplaten)*\n",
       "\n",
       "===== Document 8 =====\n",
       "## How to benchmark ğŸ¤— Transformers models\n",
       "\n",
       "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark ğŸ¤— Transformers models. The benchmark classes allow us to measure the _peak memory usage_ and _required time_ \n",
       "for both _inference_ and _training_.\n",
       "\n",
       "<Tip>\n",
       "\n",
       "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
       "backward pass.\n",
       "\n",
       "</Tip>\n",
       "\n",
       "===== Document 9 =====\n",
       "#### Ben Wang, Independent AI Researcher, EleutherAI\n",
       "- Talk: Multihost Training in Mesh Transformer JAX\n",
       "- Abstract: As models become larger, training must be scaled across multiple nodes. This talk discusses some design decisions and tradeoffs made for scaling to multiple nodes in Mesh Transformer JAX, \n",
       "a library for running model parallel transformers on TPU pods.\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 24.50 seconds| Input tokens: 4,096 | Output tokens: 359]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 24.50 seconds| Input tokens: 4,096 | Output tokens: 359]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thought:</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">The retrieved documents do not provide specific information about benchmarking Transformer models in terms of training time. Instead, they discuss different aspects of Transformer models and their </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">benchmarks, such as Table Transformer, Deformable DETR, DistilBERT, Swin2SR, and XTREME-S.</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                                                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">To get more relevant information about benchmarking Transformer model training times, I will refine the query to focus specifically on training time metrics.</span><span style=\"background-color: #0d1117\">                                           </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                                                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                   </span>\n",
       "<span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">benchmark_info </span><span style=\"color: #ff7b72; text-decoration-color: #ff7b72; background-color: #0d1117; font-weight: bold\">=</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> retriever(query</span><span style=\"color: #ff7b72; text-decoration-color: #ff7b72; background-color: #0d1117; font-weight: bold\">=</span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">\"Transformer model training time benchmark\"</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">)</span><span style=\"background-color: #0d1117\">                                                                                                                           </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(</span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">\"Information about Transformer model training time benchmark:\"</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">, benchmark_info)</span><span style=\"background-color: #0d1117\">                                                                                                                   </span>\n",
       "<span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">```</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                     </span>\n",
       "<span style=\"color: #79c0ff; text-decoration-color: #79c0ff; background-color: #0d1117; font-weight: bold\">=======</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                 </span>\n",
       "<span style=\"color: #79c0ff; text-decoration-color: #79c0ff; background-color: #0d1117; font-weight: bold\">=======</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                 </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                                                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">[{'id': 'call_4', 'type': 'function', 'function': {'name': 'python_interpreter', 'arguments': 'benchmark_info = retriever(query=\"Transformer model training time benchmark\")\\nprint(\"Information about </span><span style=\"background-color: #0d1117\"> </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Transformer model training time benchmark:\", benchmark_info)'}}]</span><span style=\"background-color: #0d1117\">                                                                                                                                        </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mOutput message of the LLM:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mThought:\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mThe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretrieved\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocuments\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnot\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprovide\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mspecific\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mabout\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbenchmarking\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodels\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mterms\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mof\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtime.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mInstead,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthey\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdiscuss\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdifferent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23maspects\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mof\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodels\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtheir\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mbenchmarks,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msuch\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mas\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTable\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformer,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mDeformable\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mDETR,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mDistilBERT,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mSwin2SR,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mXTREME-S.\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mTo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mget\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrelevant\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mabout\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbenchmarking\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtimes,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrefine\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mquery\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfocus\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mspecifically\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtime\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmetrics.\u001b[0m\u001b[48;2;13;17;23m                                           \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;165;214;255;48;2;13;17;23m```\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23mpy\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mbenchmark_info\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[1;38;2;255;123;114;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretriever\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mquery\u001b[0m\u001b[1;38;2;255;123;114;48;2;13;17;23m=\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23mTransformer model training time benchmark\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m)\u001b[0m\u001b[48;2;13;17;23m                                                                                                                           \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mprint\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23mInformation about Transformer model training time benchmark:\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbenchmark_info\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m)\u001b[0m\u001b[48;2;13;17;23m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;165;214;255;48;2;13;17;23m```\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                     \u001b[0m\n",
       "\u001b[1;38;2;121;192;255;48;2;13;17;23m=======\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                 \u001b[0m\n",
       "\u001b[1;38;2;121;192;255;48;2;13;17;23m=======\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                 \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23m[{'id':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'call_4',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'type':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'function',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'function':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m{'name':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'python_interpreter',\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'arguments':\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m'benchmark_info\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretriever(query=\"Transformer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtime\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbenchmark\")\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m\\n\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprint(\"Information\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mabout\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mTransformer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodel\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtime\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbenchmark:\",\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbenchmark_info)'}}]\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                        \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> â”€ <span style=\"font-weight: bold\">Executing this code:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">benchmark_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Transformer model training time benchmark\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                                       </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Information about Transformer model training time benchmark:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, benchmark_info)</span><span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " â”€ \u001b[1mExecuting this code:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mbenchmark_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTransformer model training time benchmark\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                       \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mInformation about Transformer model training time benchmark:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbenchmark_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Information about Transformer model training time benchmark: \n",
       "Retrieved documents:\n",
       "\n",
       "\n",
       "===== Document 0 =====\n",
       "The Table Transformer model was proposed in [PubTables-1M: Towards comprehensive table extraction from unstructured documents](https://arxiv.org/abs/2110.00061) by\n",
       "Brandon Smock, Rohith Pesala, Robin Abraham. The authors introduce a new dataset, PubTables-1M, to benchmark progress in table extraction from unstructured documents,\n",
       "\n",
       "===== Document 1 =====\n",
       "âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
       "rendered properly in your Markdown viewer.\n",
       "\n",
       "--&gt;\n",
       "\n",
       "# Time Series Transformer\n",
       "\n",
       "## Overview\n",
       "\n",
       "The Time Series Transformer model is a vanilla encoder-decoder Transformer for time series forecasting.\n",
       "This model was contributed by [kashif](https://huggingface.co/kashif).\n",
       "\n",
       "## Usage tips\n",
       "\n",
       "===== Document 2 =====\n",
       "important, getting a slightly slower training time could be a good trade.\n",
       "\n",
       "===== Document 3 =====\n",
       ". Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness \n",
       "of our approach.*\n",
       "\n",
       "===== Document 4 =====\n",
       "Distil* is a class of compressed models that started with DistilBERT. DistilBERT stands for Distilled-BERT. DistilBERT is a small, fast, cheap and light Transformer model based on Bert architecture. \n",
       "It has 40% less parameters than `bert-base-uncased`, runs 60% faster while preserving 97% of BERT's performances as measured on the GLUE language understanding benchmark\n",
       "\n",
       "===== Document 5 =====\n",
       "Instead of benchmarking pre-trained models via their model identifier, _e.g._ `bert-base-uncased`, the user can\n",
       "alternatively benchmark an arbitrary configuration of any available model class. In this case, a `list` of\n",
       "configurations must be inserted with the benchmark args as follows.\n",
       "\n",
       "&lt;frameworkcontent&gt;\n",
       "&lt;pt&gt;\n",
       "```py\n",
       "&gt;&gt;&gt; from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments, BertConfig\n",
       "\n",
       "===== Document 6 =====\n",
       "Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these\n",
       "findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the\n",
       "number of parameters at the fine-tuning stage.*\n",
       "\n",
       "===== Document 7 =====\n",
       "*We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of\n",
       "time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same \n",
       "embedding and Transformer weights across all the series\n",
       "\n",
       "===== Document 8 =====\n",
       "# Swin2SR\n",
       "\n",
       "## Overview\n",
       "\n",
       "The Swin2SR model was proposed in [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, \n",
       "Radu Timofte.\n",
       "Swin2R improves the [SwinIR](https://github.com/JingyunLiang/SwinIR/) model by incorporating [Swin Transformer v2](swinv2) layers which mitigates issues such as training instability, resolution gaps \n",
       "between pre-training\n",
       "and fine-tuning, and hunger on data.\n",
       "\n",
       "===== Document 9 =====\n",
       ". However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of \n",
       "the encoder-decoder architecture\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Information about Transformer model training time benchmark: \n",
       "Retrieved documents:\n",
       "\n",
       "\n",
       "===== Document 0 =====\n",
       "The Table Transformer model was proposed in [PubTables-1M: Towards comprehensive table extraction from unstructured documents](https://arxiv.org/abs/2110.00061) by\n",
       "Brandon Smock, Rohith Pesala, Robin Abraham. The authors introduce a new dataset, PubTables-1M, to benchmark progress in table extraction from unstructured documents,\n",
       "\n",
       "===== Document 1 =====\n",
       "âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
       "rendered properly in your Markdown viewer.\n",
       "\n",
       "-->\n",
       "\n",
       "# Time Series Transformer\n",
       "\n",
       "## Overview\n",
       "\n",
       "The Time Series Transformer model is a vanilla encoder-decoder Transformer for time series forecasting.\n",
       "This model was contributed by [kashif](https://huggingface.co/kashif).\n",
       "\n",
       "## Usage tips\n",
       "\n",
       "===== Document 2 =====\n",
       "important, getting a slightly slower training time could be a good trade.\n",
       "\n",
       "===== Document 3 =====\n",
       ". Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness \n",
       "of our approach.*\n",
       "\n",
       "===== Document 4 =====\n",
       "Distil* is a class of compressed models that started with DistilBERT. DistilBERT stands for Distilled-BERT. DistilBERT is a small, fast, cheap and light Transformer model based on Bert architecture. \n",
       "It has 40% less parameters than `bert-base-uncased`, runs 60% faster while preserving 97% of BERT's performances as measured on the GLUE language understanding benchmark\n",
       "\n",
       "===== Document 5 =====\n",
       "Instead of benchmarking pre-trained models via their model identifier, _e.g._ `bert-base-uncased`, the user can\n",
       "alternatively benchmark an arbitrary configuration of any available model class. In this case, a `list` of\n",
       "configurations must be inserted with the benchmark args as follows.\n",
       "\n",
       "<frameworkcontent>\n",
       "<pt>\n",
       "```py\n",
       ">>> from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments, BertConfig\n",
       "\n",
       "===== Document 6 =====\n",
       "Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these\n",
       "findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the\n",
       "number of parameters at the fine-tuning stage.*\n",
       "\n",
       "===== Document 7 =====\n",
       "*We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of\n",
       "time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same \n",
       "embedding and Transformer weights across all the series\n",
       "\n",
       "===== Document 8 =====\n",
       "# Swin2SR\n",
       "\n",
       "## Overview\n",
       "\n",
       "The Swin2SR model was proposed in [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, \n",
       "Radu Timofte.\n",
       "Swin2R improves the [SwinIR](https://github.com/JingyunLiang/SwinIR/) model by incorporating [Swin Transformer v2](swinv2) layers which mitigates issues such as training instability, resolution gaps \n",
       "between pre-training\n",
       "and fine-tuning, and hunger on data.\n",
       "\n",
       "===== Document 9 =====\n",
       ". However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of \n",
       "the encoder-decoder architecture\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 20.24 seconds| Input tokens: 6,144 | Output tokens: 542]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 20.24 seconds| Input tokens: 6,144 | Output tokens: 542]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Output message of the LLM:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Thoughts: The retrieved documents do not provide specific information about training time benchmarks for Transformer models. The documents focus on different aspects such as table extraction, time </span><span style=\"background-color: #0d1117\">   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">series forecasting, and image super-resolution. To get more relevant information, I will refine the query to be more specific about training time metrics for Transformer models.</span><span style=\"background-color: #0d1117\">                       </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                                                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">Code:</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                   </span>\n",
       "<span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">```py</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                   </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">benchmark_info </span><span style=\"color: #ff7b72; text-decoration-color: #ff7b72; background-color: #0d1117; font-weight: bold\">=</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\"> retriever(query</span><span style=\"color: #ff7b72; text-decoration-color: #ff7b72; background-color: #0d1117; font-weight: bold\">=</span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">\"Training time benchmark for Transformer models\"</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">)</span><span style=\"background-color: #0d1117\">                                                                                                                      </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">print(</span><span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">\"Information about training time benchmark for Transformer models:\"</span><span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">, benchmark_info)</span><span style=\"background-color: #0d1117\">                                                                                                              </span>\n",
       "<span style=\"color: #a5d6ff; text-decoration-color: #a5d6ff; background-color: #0d1117\">```</span><span style=\"background-color: #0d1117\">                                                                                                                                                                                                     </span>\n",
       "<span style=\"background-color: #0d1117\">                                                                                                                                                                                                        </span>\n",
       "<span style=\"color: #e6edf3; text-decoration-color: #e6edf3; background-color: #0d1117\">This refined query should help in retrieving documents that specifically address the training time of Transformer models.</span><span style=\"background-color: #0d1117\">                                                                               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mOutput message of the LLM:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mThoughts:\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mThe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretrieved\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocuments\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mnot\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mprovide\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mspecific\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mabout\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtime\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbenchmarks\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodels.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mThe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocuments\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfocus\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mon\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdifferent\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23maspects\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msuch\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mas\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtable\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mextraction,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtime\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[48;2;13;17;23m   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mseries\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mforecasting,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mand\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mimage\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23msuper-resolution.\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTo\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mget\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrelevant\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23minformation,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mI\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mwill\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrefine\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mquery\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mto\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmore\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mspecific\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mabout\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtime\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmetrics\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mfor\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodels.\u001b[0m\u001b[48;2;13;17;23m                       \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mCode:\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;165;214;255;48;2;13;17;23m```\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23mpy\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mbenchmark_info\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[1;38;2;255;123;114;48;2;13;17;23m=\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretriever\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mquery\u001b[0m\u001b[1;38;2;255;123;114;48;2;13;17;23m=\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23mTraining time benchmark for Transformer models\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m)\u001b[0m\u001b[48;2;13;17;23m                                                                                                                      \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mprint\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m(\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23mInformation about training time benchmark for Transformer models:\u001b[0m\u001b[38;2;165;214;255;48;2;13;17;23m\"\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m,\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mbenchmark_info\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m)\u001b[0m\u001b[48;2;13;17;23m                                                                                                              \u001b[0m\n",
       "\u001b[38;2;165;214;255;48;2;13;17;23m```\u001b[0m\u001b[48;2;13;17;23m                                                                                                                                                                                                     \u001b[0m\n",
       "\u001b[48;2;13;17;23m                                                                                                                                                                                                        \u001b[0m\n",
       "\u001b[38;2;230;237;243;48;2;13;17;23mThis\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mrefined\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mquery\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mshould\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mhelp\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23min\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mretrieving\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mdocuments\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthat\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mspecifically\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23maddress\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mthe\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtraining\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mtime\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mof\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mTransformer\u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23m \u001b[0m\u001b[38;2;230;237;243;48;2;13;17;23mmodels.\u001b[0m\u001b[48;2;13;17;23m                                                                               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> â”€ <span style=\"font-weight: bold\">Executing this code:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">benchmark_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Training time benchmark for Transformer models\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                                  </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Information about training time benchmark for Transformer models:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, benchmark_info)</span><span style=\"background-color: #272822\">                                                                                                          </span>  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " â”€ \u001b[1mExecuting this code:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mbenchmark_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTraining time benchmark for Transformer models\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                                  \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mInformation about training time benchmark for Transformer models:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbenchmark_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Information about training time benchmark for Transformer models: \n",
       "Retrieved documents:\n",
       "\n",
       "\n",
       "===== Document 0 =====\n",
       "&lt;/Tip&gt;\n",
       "\n",
       "[[open-in-colab]]\n",
       "\n",
       "Let's take a look at how ğŸ¤— Transformers models can be benchmarked, best practices, and already available benchmarks.\n",
       "\n",
       "A notebook explaining in more detail how to benchmark ğŸ¤— Transformers models can be found [here](https://github.com/huggingface/notebooks/tree/main/examples/benchmark.ipynb).\n",
       "\n",
       "## How to benchmark ğŸ¤— Transformers models\n",
       "\n",
       "===== Document 1 =====\n",
       "| Task  | Metric                       | Result      | Training time | Result (FP16) | Training time (FP16) |\n",
       "|-------|------------------------------|-------------|---------------|---------------|----------------------|\n",
       "| CoLA  | Matthews corr                | 56.53       | 3:17          | 56.78         | 1:41                 |\n",
       "| SST-2 | Accuracy                     | 92.32       | 26:06         | 91.74         | 13:11                |\n",
       "\n",
       "===== Document 2 =====\n",
       "#### Ben Wang, Independent AI Researcher, EleutherAI\n",
       "- Talk: Multihost Training in Mesh Transformer JAX\n",
       "- Abstract: As models become larger, training must be scaled across multiple nodes. This talk discusses some design decisions and tradeoffs made for scaling to multiple nodes in Mesh Transformer JAX, \n",
       "a library for running model parallel transformers on TPU pods.\n",
       "\n",
       "===== Document 3 =====\n",
       "*We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of\n",
       "time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same \n",
       "embedding and Transformer weights across all the series\n",
       "\n",
       "===== Document 4 =====\n",
       "Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these\n",
       "findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the\n",
       "number of parameters at the fine-tuning stage.*\n",
       "\n",
       "===== Document 5 =====\n",
       "Distil* is a class of compressed models that started with DistilBERT. DistilBERT stands for Distilled-BERT. DistilBERT is a small, fast, cheap and light Transformer model based on Bert architecture. \n",
       "It has 40% less parameters than `bert-base-uncased`, runs 60% faster while preserving 97% of BERT's performances as measured on the GLUE language understanding benchmark\n",
       "\n",
       "===== Document 6 =====\n",
       "## How to benchmark ğŸ¤— Transformers models\n",
       "\n",
       "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark ğŸ¤— Transformers models. The benchmark classes allow us to measure the _peak memory usage_ and _required time_ \n",
       "for both _inference_ and _training_.\n",
       "\n",
       "&lt;Tip&gt;\n",
       "\n",
       "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
       "backward pass.\n",
       "\n",
       "&lt;/Tip&gt;\n",
       "\n",
       "===== Document 7 =====\n",
       "| [How to use Benchmarks](https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| How to benchmark models with transformers | [![Open in \n",
       "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| [![Open in AWS \n",
       "Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/benchmark\n",
       "\n",
       "===== Document 8 =====\n",
       "| [How to use Benchmarks](https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| How to benchmark models with transformers | [![Open in \n",
       "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| [![Open in AWS \n",
       "Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/benchmark\n",
       "\n",
       "===== Document 9 =====\n",
       "[[autodoc]] data.processors.utils.InputExample\n",
       "\n",
       "[[autodoc]] data.processors.utils.InputFeatures\n",
       "\n",
       "## GLUE\n",
       "\n",
       "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) is a benchmark that evaluates the\n",
       "performance of models across a diverse set of existing NLU tasks. It was released together with the paper [GLUE: A\n",
       "multi-task benchmark and analysis platform for natural language understanding](https://openreview.net/pdf?id=rJ4km2R5t7)\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Information about training time benchmark for Transformer models: \n",
       "Retrieved documents:\n",
       "\n",
       "\n",
       "===== Document 0 =====\n",
       "</Tip>\n",
       "\n",
       "[[open-in-colab]]\n",
       "\n",
       "Let's take a look at how ğŸ¤— Transformers models can be benchmarked, best practices, and already available benchmarks.\n",
       "\n",
       "A notebook explaining in more detail how to benchmark ğŸ¤— Transformers models can be found [here](https://github.com/huggingface/notebooks/tree/main/examples/benchmark.ipynb).\n",
       "\n",
       "## How to benchmark ğŸ¤— Transformers models\n",
       "\n",
       "===== Document 1 =====\n",
       "| Task  | Metric                       | Result      | Training time | Result (FP16) | Training time (FP16) |\n",
       "|-------|------------------------------|-------------|---------------|---------------|----------------------|\n",
       "| CoLA  | Matthews corr                | 56.53       | 3:17          | 56.78         | 1:41                 |\n",
       "| SST-2 | Accuracy                     | 92.32       | 26:06         | 91.74         | 13:11                |\n",
       "\n",
       "===== Document 2 =====\n",
       "#### Ben Wang, Independent AI Researcher, EleutherAI\n",
       "- Talk: Multihost Training in Mesh Transformer JAX\n",
       "- Abstract: As models become larger, training must be scaled across multiple nodes. This talk discusses some design decisions and tradeoffs made for scaling to multiple nodes in Mesh Transformer JAX, \n",
       "a library for running model parallel transformers on TPU pods.\n",
       "\n",
       "===== Document 3 =====\n",
       "*We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of\n",
       "time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same \n",
       "embedding and Transformer weights across all the series\n",
       "\n",
       "===== Document 4 =====\n",
       "Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these\n",
       "findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the\n",
       "number of parameters at the fine-tuning stage.*\n",
       "\n",
       "===== Document 5 =====\n",
       "Distil* is a class of compressed models that started with DistilBERT. DistilBERT stands for Distilled-BERT. DistilBERT is a small, fast, cheap and light Transformer model based on Bert architecture. \n",
       "It has 40% less parameters than `bert-base-uncased`, runs 60% faster while preserving 97% of BERT's performances as measured on the GLUE language understanding benchmark\n",
       "\n",
       "===== Document 6 =====\n",
       "## How to benchmark ğŸ¤— Transformers models\n",
       "\n",
       "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark ğŸ¤— Transformers models. The benchmark classes allow us to measure the _peak memory usage_ and _required time_ \n",
       "for both _inference_ and _training_.\n",
       "\n",
       "<Tip>\n",
       "\n",
       "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
       "backward pass.\n",
       "\n",
       "</Tip>\n",
       "\n",
       "===== Document 7 =====\n",
       "| [How to use Benchmarks](https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| How to benchmark models with transformers | [![Open in \n",
       "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| [![Open in AWS \n",
       "Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/benchmark\n",
       "\n",
       "===== Document 8 =====\n",
       "| [How to use Benchmarks](https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| How to benchmark models with transformers | [![Open in \n",
       "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| [![Open in AWS \n",
       "Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/benchmark\n",
       "\n",
       "===== Document 9 =====\n",
       "[[autodoc]] data.processors.utils.InputExample\n",
       "\n",
       "[[autodoc]] data.processors.utils.InputFeatures\n",
       "\n",
       "## GLUE\n",
       "\n",
       "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) is a benchmark that evaluates the\n",
       "performance of models across a diverse set of existing NLU tasks. It was released together with the paper [GLUE: A\n",
       "multi-task benchmark and analysis platform for natural language understanding](https://openreview.net/pdf?id=rJ4km2R5t7)\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 15.21 seconds| Input tokens: 8,192 | Output tokens: 658]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 15.21 seconds| Input tokens: 8,192 | Output tokens: 658]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Reached max steps.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mReached max steps.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Final answer: In transformer models, the backward pass is typically slower than the forward pass. The backward pass involves calculating gradients through backpropagation, which can be computationally\n",
       "intensive and time-consuming compared to the forward pass, where only inference is performed. The provided documents indicate that both training and inference times are measured in benchmarks, with \n",
       "training time encompassing both forward and backward passes.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Final answer: In transformer models, the backward pass is typically slower than the forward pass. The backward pass involves calculating gradients through backpropagation, which can be computationally\n",
       "intensive and time-consuming compared to the forward pass, where only inference is performed. The provided documents indicate that both training and inference times are measured in benchmarks, with \n",
       "training time encompassing both forward and backward passes.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 0.00 seconds| Input tokens: 10,240 | Output tokens: 732]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 4: Duration 0.00 seconds| Input tokens: 10,240 | Output tokens: 732]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output:\n",
      "In transformer models, the backward pass is typically slower than the forward pass. The backward pass involves calculating gradients through backpropagation, which can be computationally intensive and time-consuming compared to the forward pass, where only inference is performed. The provided documents indicate that both training and inference times are measured in benchmarks, with training time encompassing both forward and backward passes.\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
    "    for doc in filtered_knowledge_base\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "docs_processed = text_splitter.split_documents(source_docs)\n",
    "\n",
    "from smolagents import Tool\n",
    "\n",
    "\n",
    "class RetrieverTool(Tool):\n",
    "    name = \"retriever\"\n",
    "    description = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, docs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.retriever = BM25Retriever.from_documents(docs, k=10)\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        docs = self.retriever.invoke(\n",
    "            query,\n",
    "        )\n",
    "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
    "            [\n",
    "                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n",
    "                for i, doc in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "from smolagents import HfApiModel, CodeAgent, LiteLLMModel\n",
    "\n",
    "model_id = \"ollama/qwen2.5-coder:14b\"\n",
    "model = LiteLLMModel(model_id=model_id, api_base=\"http://localhost:11434\")\n",
    "\n",
    "retriever_tool = RetrieverTool(docs_processed)\n",
    "agent = CodeAgent(\n",
    "    tools=[retriever_tool],\n",
    "    model=model,\n",
    "    max_steps=4,\n",
    "    verbosity_level=2,\n",
    "    add_base_tools=True,\n",
    ")\n",
    "\n",
    "agent_output = agent.run(\n",
    "    \"For a transformers model training, which is slower, the forward or the backward pass?\"\n",
    ")\n",
    "\n",
    "print(\"Final output:\")\n",
    "print(agent_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
